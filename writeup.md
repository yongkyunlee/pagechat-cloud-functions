## Inspiration
Have either of the following happened to you?
* Ever since elementary school you've been fascinated by 17th century Turkish ethnography. Luckily, you just discovered a preeminent historian's blog about the collapse of the Ottoman Empire. Overjoyed, you start to text your friends, but soon remember that they're into 19th century Victorian poetry. If only you could share your love of historical discourse with another intellectual.
* Because you're someone with good taste, you're browsing Buzzfeed. Somehow "27 Extremely Disturbing Wikipedia Pages That Will Haunt Your Dreams" is not cutting it for you. Dang. If only you could see what your best friend Alicia was browsing. She would definitely know how to help you procrastinate on your TreeHacks project. 

Great! Because we built Pagechat for you. We all have unique interests, many of which are expressed through our internet browsing. We believe that building simple connections through those interests is a powerful way to improve well-being. We built an _convenient_ and _efficient_ tool to connect people through their internet browsing.
## What it does
Pagechat is a Google Chrome extension designed to promote serendipituous connections by offering one-on-one text chats centered on internet browsing. When active, Pagechat
* displays what you are your friends are currently reading, allowing you to discover and share interesting articles
* centers the conversation around webpages by giving friends the opportunity to chat each other directly through Chrome
* intelligently connects users with similar interests by creating one-on-one chats for users visiting the same webpage

## How we built it
### Chatting
### User Recommendations
If several people are browsing the same website and want to chat with each other, how do we pair them up? Intuitively, people who have similar browsing histories will have more in common to talk about, so we should group them together. We maintain a dynamic **feature vector** for each user, which is based off of their reading history. We want feature vectors with small cosine distance to be similar.

When is active on Pagechat and visits a site on our whitelist (we don't want sites with generic titles, so we stick to mostly news sites), we obtain the title of the page. We make the assumption that the title is representative of the content our user is reading. For example, we would expect that "27 Extremely Disturbing Wikipedia Pages That Will Haunt Your Dreams" has different content from "Ethnography Museum of Ankara". To obtain a reasonable embedding of our title, we use [SBERT](https://arxiv.org/abs/1908.10084), a BERT-based language model trained to predict the representation of sentences. SBERT can attend to the salient keywords in each title and can obtain a global representation of the title.

Next, we need some way to update feature vectors whenever a user visits a new page. This is well-suited for a recurrent neural network. These models maintain a hidden state that is continually updated with each new query. We will use an [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) to update our feature vectors. It takes in the previous feature vector and new title and outputs the new feature vector. 

Finally, we need to train our LSTM. Fortunately, UCI has released a [dataset](https://archive.ics.uci.edu/ml/datasets/News+Aggregator) of news headlines along with their respective category (business, science and technology, entertainment, health). We feed these headlines as training data for the model. Before training, we preprocess the headlines, embedding all of them with SBERT. This greatly reduces training times. We use the following three-step procedure to train the model:
1. _Train an [autoencoder](https://arxiv.org/abs/1502.04681) to learn a compressed representation of the feature title._ SBERT outputs features vectors with 768 elements, which is large and unwieldy. An autoencoder is an unsupervised learning model that is able to learn high-fidelity compressed representations of the input sequence. The encoder (an LSTM) encodes a sequence of headlines into a lower-dimensional (128) space and the decoder (another LSTM) decodes the sequence. The model's goal is to output a sequence as close as possible to the original input sequence. After training, we will end up with an encoder LSTM that is able to faithfully map 768 element input vector to a 128 element space and maintain the information of the representation. We use a [contractive autoencoder](https://icml.cc/2011/papers/455_icmlpaper.pdf), which adds an extra loss term to promote the encoder to be less sensitive to variance in the input.
2. _Train the encoder to condense feature vectors that share a category._ Suppose that Holo reads several business articles and Lawrence also reads several business articles. Ideally, they should have similar feature vectors. To train, we build sequences of 5 headlines, where each headline in a sequence is drawn from the same category. The encoder LSTM from the previous step encodes these sequences to feature vectors. We train it to obtain a higher cosine similarity for vectors that share a category than for vectors that don't.
3. _Train the encoder to condense feature vectors that share a story._ The dataset also has a feature corresponding to the specific news story an article covers. Similar to step 2, we build sequences of 5 headlines, where each headline in a sequence is drawn from the same story. By training the encoder to predict a high cosine similarity for vectors that share a story, we further improve the representation of the feature vectors.

We provide some evaluations for our model and document our training process in this [Google notebook](https://colab.research.google.com/drive/1B4eWINVyWntF0VESUUgRt2opA4QFoS-M?usp=sharing). Feel free to make a copy and tinker with it!
## What's next for Pagechat
* Train a forgetful LSTM